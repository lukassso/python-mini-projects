{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5254,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":4039,"modelId":2195},{"sourceId":170872,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":145402,"modelId":167966}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Few-Shot Medical Image Classification with RAG and VLMs\n\nThis lab introduces you to the medical applications of retrieval-augmented generation (RAG). We will be working on a brain tumor classifier. For a classifier, we are going to use a vision-language model (VLM). We will make the model classify the images through a prompt enhanced with 5 examples of most similar images with labels to the analyzed image. To retieve similar images, we will setup a Qdrant vector datasets of brain tumor MRI images. Each image will be encoded in the database with the medical encoder MedImageInsight released by Microsoft.\n\nThis approach does not require any training of the classifier. The positive effect of supplying VLM with similar images is most apparent for smaller models.\n\n### Key Concepts\n\n* **Zero-Shot Learning**\n\nA machine learning paradigm where models can recognize or classify objects they haven't been explicitly trained on. Particularly valuable in medical contexts where labeled data may be scarce. Relies on transferring knowledge from related tasks and understanding semantic relationships.\n\n\n* **RAG (Retrieval-Augmented Generation)**\n\nA hybrid approach that combines information retrieval from a knowledge base (in our case, a database of medical images) and large language model generation capabilities.\n\nEnhances model performance by providing relevant context before making decisions. Improves reliability by grounding predictions in similar historical cases\n\n\n* **VLM (Vision Language Models)**\n\nAdvanced AI models that can process both images and text. Can understand and describe medical images in natural language. Enable more interpretable and explainable AI decisions in medical contexts.\n\n\n* **Vector Databases**\n\nSpecialized databases that store high-dimensional vector representations of images. Enable efficient similarity search using distance metrics\nCritical for retrieving relevant historical cases to support decision-making\n\n\n\n### Laboratory Objectives\nIn this lab, you will:\n\n1. Set up a RAG framework for medical image analysis\n2. Create and manage a vector database of brain MRI scans\n3. Implement zero-shot classification using state-of-the-art VLMs\n4. Evaluate system performance using various metrics (homework).\n\n### Prerequisites\n\n* Basic understanding of Python programming\n* Familiarity with deep learning concepts\n* Knowledge of medical imaging basics\n\n### Tools and Libraries\n\n* Python 3.10+\n* UMIE datasets for the images\n* Transformers library for VLMs (they recently released a module supporting)\n* Qdrant for vector database management\n* MedImageInsight model for medical image encoding of images in the vector database","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup","metadata":{}},{"cell_type":"markdown","source":"### 1.1 Install required libraries","metadata":{}},{"cell_type":"code","source":"!pip install qdrant_client\n!pip install einops transformers_stream_generator\n!pip install datasets\n!pip install transformers accelerate -U","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.2 Load Brain Tumor Classification dataset from UMIE datasets repo on HuggingFace","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"lion-ai/umie_datasets\", \"brain_tumor_classification\", split='train')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.3 Select a single image from each study. The selected image should have the most tumor visible.","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nfrom typing import Dict, List, Optional, Union\nimport re\n\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset, DatasetDict, load_dataset\nfrom sklearn.model_selection import train_test_split\n\n\ndef group_by_study_id(dataset: Dataset) -> Dict[str, List[Dict]]:\n    \"\"\"Group images by study_id.\"\"\"\n    study_groups = defaultdict(list)\n    for item in dataset:\n        study_groups[item[\"study_id\"]].append(item)\n    return dict(study_groups)\n\n\ndef get_study_label(study_group: List[Dict]) -> str:\n    \"\"\"Get the dominant source label for a study group.\"\"\"\n    label_counts: dict = defaultdict(int)\n    for item in study_group:\n        label = item[\"labels\"]\n        pattern = r'[a-zA-Z]+'\n        label = ''.join(re.findall(pattern, label))\n        label_counts[label] += 1\n    return max(label_counts.items(), key=lambda x: x[1])[0]\n\n\ndef calculate_val_size(study_df: pd.DataFrame, val_size: Union[float, int], total_images: int) -> float:\n    \"\"\"\n    Calculate validation set size as a proportion based on input specification.\n\n    Args:\n        study_df: DataFrame containing study information\n        val_size: Either a float (proportion) or int (absolute number)\n        total_images: Total number of images in the dataset\n\n    Returns:\n        Float representing the proportion for validation set\n    \"\"\"\n    if isinstance(val_size, float):\n        if not 0 < val_size < 1:\n            raise ValueError(\"Validation proportion must be between 0 and 1\")\n        return val_size\n    elif isinstance(val_size, int):\n        if not 0 < val_size < total_images:\n            raise ValueError(f\"Validation count must be between 0 and {total_images}\")\n        return val_size / total_images\n    else:\n        raise ValueError(\"val_size must be either float (proportion) or int (count)\")\n\n\ndef create_split(\n    dataset: Dataset, \n    val_size: Union[float, int] = 0.2, \n    random_state: Optional[int] = 42\n) -> DatasetDict:\n    \"\"\"\n    Create train-val split while keeping same study_id together and stratifying based on source_labels.\n\n    Args:\n        dataset: Hugging Face dataset\n        val_size: Either a float between 0-1 (proportion) or int (exact number of examples)\n        random_state: Random seed for reproducibility\n\n    Returns:\n        DatasetDict containing train and validation splits\n    \"\"\"\n    # Group data by study_id\n    study_groups = group_by_study_id(dataset)\n\n    # Create a list of (study_id, dominant_label, group_size) tuples\n    study_info = [\n        (study_id, get_study_label(group), len(group)) \n        for study_id, group in study_groups.items()\n    ]\n\n    # Convert to DataFrame for easier manipulation\n    study_df = pd.DataFrame(study_info, columns=[\"study_id\", \"label\", \"group_size\"])\n\n    total_images = len(dataset)\n    val_proportion = calculate_val_size(study_df, val_size, total_images)\n\n    # Perform stratified split on study level\n    train_studies, val_studies = train_test_split(\n        study_df[\"study_id\"],\n        test_size=val_proportion,\n        random_state=random_state,\n        stratify=study_df[\"label\"]\n    )\n\n    # Convert to sets for faster lookup\n    train_studies_set = set(train_studies)\n\n    # Create train and validation splits\n    train_indices = [\n        i for i, item in enumerate(dataset) \n        if item[\"study_id\"] in train_studies_set\n    ]\n    val_indices = [\n        i for i, item in enumerate(dataset) \n        if item[\"study_id\"] not in train_studies_set\n    ]\n\n    # Create the splits using Dataset.select()\n    train_dataset = dataset.select(train_indices)\n    val_dataset = dataset.select(val_indices)\n\n    # Print split statistics\n    print(\"\\nSplit Statistics:\")\n    print(f\"Total studies: {len(study_groups)}\")\n    print(f\"Total images: {total_images}\")\n    print(f\"Train studies: {len(train_studies)}\")\n    print(f\"Val studies: {len(val_studies)}\")\n    print(f\"Train images: {len(train_dataset)}\")\n    print(f\"Val images: {len(val_dataset)}\")\n\n    # Validate if using exact number\n    if isinstance(val_size, int):\n        print(f\"\\nRequested validation size: {val_size}\")\n        print(f\"Actual validation size: {len(val_dataset)}\")\n        print(\"Note: Actual size might differ slightly from requested size due to study grouping\")\n\n    return DatasetDict({\n        \"train\": train_dataset,\n        \"validation\": val_dataset\n    })\n\n\nsplits = create_split(dataset=dataset, val_size=100, random_state=42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.3 Copy MedImageInsight model directory to current directory","metadata":{}},{"cell_type":"code","source":"!cp -a /kaggle/input/medimageinsight/pytorch/default/1/. /kaggle/working/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1. 4 Install model requirements","metadata":{}},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Setup RAG Framework","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Create a medical encoder that will be used to create embedding for the vector database","metadata":{}},{"cell_type":"code","source":"from medimageinsightmodel import MedImageInsight","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vision_encoder = MedImageInsight(\n    model_dir=\"2024.09.27\",\n    vision_model_name=\"medimageinsigt-v1.0.0.pt\",\n    language_model_name=\"language_model.pth\"\n)\nvision_encoder.load_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.2 Setup Vector Database Client","metadata":{}},{"cell_type":"code","source":"from qdrant_client import QdrantClient\n\nclient = QdrantClient(\":memory:\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.3 Labels used by the Brain Tumor Classification dataset (multiclass classification problem)","metadata":{}},{"cell_type":"code","source":"labels = [\n    \"Normal\",\n    \"Glioma\",\n    \"Meningioma\",\n    \"Pituitary\"\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.4 Helper functions for creating image embeddings ","metadata":{}},{"cell_type":"code","source":"import base64\nfrom io import BytesIO\nfrom PIL import Image\nimport numpy as np\n\n\ndef convert_to_base64(image, resize=None, max_size=512):\n    # Check if the image is a numpy array\n    if isinstance(image, np.ndarray):\n        # Convert the numpy array to a PIL Image\n        image = Image.fromarray(image)\n\n    # Resize the image if a new size is specified\n    if resize:\n        if isinstance(resize, tuple) and len(resize) == 2:\n            image = image.resize(resize, Image.LANCZOS)\n        else:\n            raise ValueError(\"Resize parameter must be a tuple of (width, height)\")\n\n    # Scale the image to fit within max_size while preserving aspect ratio\n    if max_size:\n        if isinstance(max_size, int) and max_size > 0:\n            image.thumbnail((max_size, max_size), Image.LANCZOS)\n        else:\n            raise ValueError(\"Max size must be a positive integer\")\n\n    buffered = BytesIO()\n    image.save(buffered, format=\"PNG\")\n    img_str = base64.b64encode(buffered.getvalue())\n    return img_str.decode(\"utf-8\")\n\ndef resize_base64(base64_string, resize=None, max_size=512):\n    # Decode the base64 string to image\n    image_data = base64.b64decode(base64_string)\n    image = Image.open(BytesIO(image_data))\n\n    # Resize the image if a new size is specified\n    if resize:\n        if isinstance(resize, tuple) and len(resize) == 2:\n            image = image.resize(resize, Image.LANCZOS)\n        else:\n            raise ValueError(\"Resize parameter must be a tuple of (width, height)\")\n\n    # Scale the image to fit within max_size while preserving aspect ratio\n    if max_size:\n        if isinstance(max_size, int) and max_size > 0:\n            image.thumbnail((max_size, max_size), Image.LANCZOS)\n        else:\n            raise ValueError(\"Max size must be a positive integer\")\n\n    # Convert the resized image back to base64\n    buffered = BytesIO()\n    image.save(buffered, format=\"PNG\")\n    resized_base64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n\n    return resized_base64\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.5 Create a class for creating and managing the vector database of images end labels","metadata":{}},{"cell_type":"code","source":"import cv2\nfrom qdrant_client.models import PointStruct, VectorParams, Distance\nimport os\n\n\nclass Retriever:\n    def __init__(self, encoder, collection_name: str, n_images: int = 5):\n        self.encoder = encoder\n        self.client = client\n        self.collection_name = collection_name\n        self.n_images = n_images\n\n    def get_similar_imgs(self, img_str: list[str]):\n        embedings = self.encoder.encode(img_str)[0]\n\n        search_result = self.client.query_points(\n            collection_name=self.collection_name,\n            query=embedings,\n            with_payload=True,\n            limit=self.n_images,\n        ).points\n\n        results = []\n        for p in search_result:\n            results.append(\n                {\n                    \"id\": p.id,\n                    \"data\": p.payload,\n                }\n            )\n\n        return results\n\n    def _create_id(self, umie_path):\n        return int(os.path.basename(umie_path)[0:-4].replace(\"_\", \"0\").lstrip(\"0\"))\n\n    def _upload_batch(self, batch):\n        img_paths = batch[\"umie_id\"]\n        labels = batch[\"labels\"]\n\n        images = []\n\n        for i in range(len(batch['image'])):\n            base_img = convert_to_base64(batch[\"image\"][i])\n            images.append(base_img)\n        img_embeddings = self.encoder.encode(images)\n\n        points = []\n        ids = []\n\n        for img_path, img_embedding, img, label in zip(\n            img_paths, img_embeddings, images, labels\n        ):\n            try:\n                id = self._create_id(img_path)\n                if id in ids:\n                    print(f\"error: {id}\")\n                    print(f\"error: {img_path}\")\n                    break\n                ids.append(id)\n                label = label[0]\n                points.append(\n                    PointStruct(\n                        id=id,\n                        vector=img_embedding,\n                        payload={\"label\": label, \"id\": id, \"img\": img},\n                    )\n                )\n            except Exception as e:\n                print(f\"error: {img_path} {e}\")\n\n        operation_info = self.client.upsert(\n            collection_name=self.collection_name, wait=True, points=points\n        )\n\n    def upload_dataset(self, dataset, batch_size=8, replace_collection=False):\n        if client.collection_exists(self.collection_name):\n            if replace_collection:\n                client.delete_collection(self.collection_name)\n\n                client.create_collection(\n                    collection_name=self.collection_name,\n                    vectors_config=VectorParams(\n                        size=1024, distance=Distance.COSINE\n                    ),\n                )\n        else:\n            client.create_collection(\n                collection_name=self.collection_name,\n                vectors_config=VectorParams(\n                    size=1024, distance=Distance.COSINE\n                ),\n            )\n\n        dataset = dataset.select(range(20))\n        dataset.map(self._upload_batch, batch_size=batch_size, batched=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.6 Store brain tumor classification dataset in the vector database. Embed the images with MedImageInsight Encoder","metadata":{}},{"cell_type":"markdown","source":"*We selected validation split for demonstration purposes, not to use too much memory on Kaggle. You should normally use \"train\" split.","metadata":{}},{"cell_type":"code","source":"retriever = Retriever(encoder=vision_encoder, collection_name= \"brain_tumor_classification\", n_images= 5)\nretriever.upload_dataset(splits[\"validation\"], batch_size=8, replace_collection=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Create Medical Image Analyzer","metadata":{}},{"cell_type":"markdown","source":"### 3.1. Create prompts. We want our Vision Langauge Model to classify brain tumor type. Before performing classification, we supply VLM with 5 most similar images retrieved from the database to ground it before making a prediction.","metadata":{}},{"cell_type":"code","source":"system_message = \"\"\"You are a medical expert.\nAnalize the MRI image and classify if there is a tumor present.\nSelect the appropriate class from {labels}.\"\"\"\n\nfinal_rag_message = \"\"\"\nNow, please analyze the new image and provide your classification.\nYou always need to classify.\nReturn only the result in the json format: {'y_pred': y_pred, 'explanation': explanation}.\nThe explanation should be brief.\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.2 Set up VLM. We are going to use Qwen 2.5 B","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\npipe = pipeline(\"image-text-to-text\", model=\"Qwen/Qwen2-VL-7B-Instruct\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Alternatively, you can try VLMs hosted on Kaggle.","metadata":{}},{"cell_type":"code","source":"# from transformers import pipeline\n# pipe = pipeline(\"text-generation\",\"/kaggle/input/qwen/pytorch/vl/1\", trust_remote_code=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.3 Create a class for analyzing images, creating a stuctured prompt and combining the retriever with VLM.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport requests\nfrom litellm import completion\nimport re\n\nclass ImageAnalyzer:\n    def __init__(self, dataset, model):\n        self.dataset = dataset\n        self.model = model\n        self.add_classes = add_classes\n\n    def _create_base_prompt(self, modality: str, labels: list) -> str:\n        prompt = system_message.format(modality=modality, labels=labels)\n\n        return prompt\n\n    def _create_few_shot_message(self, similar_images):\n        messages = []\n\n        for example in similar_images:\n            image = (\n                example[\"data\"][\"img\"]\n                if \"img\" in example[\"data\"]\n                else example[\"data\"][\"image\"]\n            )\n            img = resize_base64(image)\n\n                label = example[\"data\"][\"label\"]\n\n                prompt = f\"\"\"Example:\n                                class: {label}\n                                    \"\"\"\n            message = {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": prompt,\n                    },\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": f\"data:image/png;base64,{img}\"},\n                    },\n                ],\n            }\n            messages.append(message)\n\n        return messages\n\n    def _description_message(self, modality, classes, few_shot=False):\n        prompt = self._create_base_prompt(modality, classes)\n\n        if few_shot:\n            prompt += \"\\n Here are some examples to learn from:\"\n\n        message = [\n            {\n                \"role\": \"system\",\n                \"content\": prompt,\n            }\n        ]\n        return message\n\n    def _create_rag_final_message(self, img_str):\n        message = {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": final_rag_message},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/png;base64,{img_str}\"},\n                },\n            ],\n        }\n        return message\n\n    def _get_message(self, img_str, similar_images=[]):\n        modality = self.dataset.modality\n        classes = self.dataset.classes\n        few_shot = True if len(similar_images) > 0 else False\n        messages = self._description_message(modality, classes, few_shot)\n        if len(similar_images) > 0:\n            messages.extend(self._create_few_shot_message(similar_images))\n        messages.append(self._create_rag_final_message(img_str))\n\n        return messages\n\n    def _get_aux_imgs_info(self, similar_imgs):\n        aux_images = {}\n        for idx, example in enumerate(similar_imgs):\n            aux_images[f\"aux_{idx}_id\"] = (example[\"data\"][\"id\"],)\n            if self.add_classes:\n                aux_images[f\"aux_{idx}_label\"] = example[\"data\"][\"label\"]\n        return aux_images\n\n    def analyze_img(self, img_str, similar_imgs, verbose=False):\n        messages = self._get_message(img_str, similar_imgs)\n\n        response = completion(\n            model=self.model.name, api_key=self.model.api_key, messages=messages\n        )\n        \n        outputs = pipe(text=messages, max_new_tokens=40, return_full_text=False)\n        result = outputs[0][\"generated_text\"]\n        if verbose:\n            print(result)\n        \n        return result\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_analyzer = ImageAnalyzer(dataset, model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Homework\nTest your system on the validation set. You can also try using different VLMs and different datasets from UMIE.\nCompare the following metrics:\n\n- Accuracy\n- Confidence scores\n- Classification speed\n- Quality of explanations\n\n\nCreate a summary report of your findings:\n```python def evaluate_system(analyzer, test_data):\n    results = {\n        'accuracy': [],\n        'confidence': [],\n        'speed': [],\n        'explanation_quality': []\n    }\n    return results\n```","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}