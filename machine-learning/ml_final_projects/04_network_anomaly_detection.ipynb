{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8fba0f8",
   "metadata": {},
   "source": [
    "# Task 4: Network Anomaly Detection using a Deep Autoencoder\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "**Objective:**\n",
    "The primary goal of this project is to develop and evaluate a deep autoencoder model for detecting anomalies in network traffic. The model will be trained to distinguish between normal network connections and various types of malicious attacks, such as Denial-of-Service (DoS), port scanning, and unauthorized access attempts.\n",
    "\n",
    "**Dataset:**\n",
    "The project utilizes the **KDD Cup 1999 dataset**, which was created by MIT Lincoln Labs for intrusion detection system evaluations. We will be working with the `kddcup.data_10_percent.gz` subset, which contains a large number of network connection records. Each record is described by 41 features and is labeled as either `normal.` or a specific type of attack.\n",
    "\n",
    "**Methodology:**\n",
    "The core approach is to build an autoencoder, a type of neural network trained to reconstruct its input data. The key steps of the methodology are:\n",
    "1.  **Data Loading and Preprocessing:** Load the dataset, assign correct column names, and perform necessary preprocessing, including scaling numerical features and encoding categorical ones.\n",
    "2.  **Model Architecture:** Design a deep autoencoder with multiple dense layers for both the encoder and the decoder.\n",
    "3.  **Training Strategy:** Crucially, the autoencoder will be trained **exclusively on data corresponding to 'normal' network traffic**. The underlying hypothesis is that the model will learn to reconstruct normal data with a low error, but will struggle to reconstruct anomalous data (attacks), resulting in a high reconstruction error.\n",
    "4.  **Evaluation:** The reconstruction error will serve as an anomaly score. By setting an appropriate threshold on this error, we can classify connections as either normal or anomalous. The model's performance will be evaluated on a test set containing both normal and anomalous data using metrics such as the confusion matrix, Receiver Operating Characteristic (ROC) curve, and the Area Under the Curve (AUC).\n",
    "\n",
    "**Tools and Libraries:**\n",
    "*   **Python 3.x**\n",
    "*   **Pandas & NumPy** for data manipulation.\n",
    "*   **Scikit-learn** for data preprocessing (scaling, splitting).\n",
    "*   **TensorFlow/Keras** for building and training the deep autoencoder model.\n",
    "*   **Matplotlib & Seaborn** for data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c29ab9",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e2c5182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef7660",
   "metadata": {},
   "source": [
    "### 1. Data Loading and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc8b9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded column names: 42\n",
      "List of column names:\n",
      "['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'outcome']\n",
      "Data loaded. DataFrame dimensions:\n",
      "(494021, 42)\n",
      "Number of unique values in the 'outcome' column: 23\n",
      "\n",
      "Counts of each transmission type:\n",
      "outcome\n",
      "smurf.              280790\n",
      "neptune.            107201\n",
      "normal.              97278\n",
      "back.                 2203\n",
      "satan.                1589\n",
      "ipsweep.              1247\n",
      "portsweep.            1040\n",
      "warezclient.          1020\n",
      "teardrop.              979\n",
      "pod.                   264\n",
      "nmap.                  231\n",
      "guess_passwd.           53\n",
      "buffer_overflow.        30\n",
      "land.                   21\n",
      "warezmaster.            20\n",
      "imap.                   12\n",
      "rootkit.                10\n",
      "loadmodule.              9\n",
      "ftp_write.               8\n",
      "multihop.                7\n",
      "phf.                     4\n",
      "perl.                    3\n",
      "spy.                     2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set pandas display options to show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# List to store column names\n",
    "column_names = []\n",
    "\n",
    "# Load column names from the kddcup.names file\n",
    "with open('kddcup.names', 'r') as f:\n",
    "    for line in f:\n",
    "        # Skip lines that do not contain column descriptions\n",
    "        if ':' in line:\n",
    "            # Split the line by ':' and take the first part as the column name\n",
    "            name = line.split(':')[0]\n",
    "            column_names.append(name)\n",
    "\n",
    "# Add the target column name at the end, which is not described among the features\n",
    "column_names.append('outcome')\n",
    "\n",
    "print(f\"Number of loaded column names: {len(column_names)}\")\n",
    "print(\"List of column names:\")\n",
    "print(column_names)\n",
    "\n",
    "# Path to the data file\n",
    "file_path = 'kddcup.data_10_percent.gz'\n",
    "\n",
    "# Load data into a pandas DataFrame\n",
    "df = pd.read_csv(file_path, header=None, names=column_names, compression='gzip')\n",
    "\n",
    "print(\"Data loaded. DataFrame dimensions:\")\n",
    "print(df.shape)\n",
    "\n",
    "# Check unique values and their counts in the 'outcome' column\n",
    "outcome_counts = df['outcome'].value_counts()\n",
    "\n",
    "print(f\"Number of unique values in the 'outcome' column: {len(outcome_counts)}\")\n",
    "print(\"\\nCounts of each transmission type:\")\n",
    "print(outcome_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "overview-of-data-science-tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
